#BOX BLUR

ğŸš€ CUDA Particle Filter â€“ GPU Capstone Project
ğŸ“Œ Overview

This project implements a CUDA-accelerated Particle Filter for real-time state estimation and tracking.
The goal is to demonstrate how GPU parallelism can drastically speed up computationally expensive algorithms by processing thousands of particles simultaneously.

All major stages of the particle filter are executed on the GPU:

Initialization

Propagation

Weight update

Normalization

Resampling

This project was developed as part of the GPU Specialization Capstone.

ğŸ§  Algorithm Summary

A particle filter estimates the state of a system using a large set of random samples (particles).
Each iteration performs:

Propagation
Updates each particle using a motion model.

Update
Computes the likelihood of each particle based on sensor measurements.

Normalization
Normalizes all particle weights so they sum to 1.

Resampling
Generates a new particle set based on weight distribution.

GPU parallelism assigns one thread per particle, enabling massive speedup over CPU implementations.

ğŸ“‚ Project Structure
.
â”œâ”€â”€ main.cu                # Program entry point
â”œâ”€â”€ particle_filter.cu     # CUDA kernel implementations
â”œâ”€â”€ particle_filter.cuh    # Structs and kernel declarations
â”œâ”€â”€ pgm_images.zip         # Test PGM images (synthetic data)
â””â”€â”€ README.md

âš™ï¸ Requirements

NVIDIA GPU

CUDA Toolkit (12.4+ recommended)

Linux / Google Colab / WSL (recommended)

nvcc compiler

ğŸ”§ Compilation

Navigate to project folder and run:

nvcc -o particle_filter main.cu particle_filter.cu

â–¶ï¸ Execution

Run:

./particle_filter


If your program takes arguments:

./particle_filter input_file particle_count

ğŸ“Š Sample Output
Init time (ms): 11.3092
Iter 0 â€” prop: 0.007 ms, update: 0.003 ms, norm: 0.003 ms, resamp: 0.003 ms
Iter 1 â€” prop: 0.003 ms, update: 0.004 ms, norm: 0.003 ms, resamp: 0.003 ms
...
Iter 9 â€” prop: 0.004 ms, update: 0.003 ms, norm: 0.003 ms, resamp: 0.003 ms
Done.

Interpretation

Init time â†’ GPU memory setup and particle initialization

prop â†’ propagation kernel

update â†’ weight update kernel

norm â†’ normalization kernel

resamp â†’ resampling kernel

Each iteration completes in ~0.012 ms, proving GPU acceleration efficiency.

ğŸ–¼ Test Data

The repository includes 10 synthetic PGM images used for testing:

256Ã—256 grayscale images

Stored in pgm_images.zip

Useful for benchmarking and validation

ğŸ“ˆ Performance Highlights

Massive parallelism: one thread per particle

Sub-millisecond kernel execution

Scales efficiently with particle count

Minimal host-device memory transfers

ğŸ§ª What I Learned

CUDA kernel design

GPU memory management (cudaMalloc, cudaMemcpy)

Kernel launch configuration

Parallel reduction techniques

Performance profiling

GPU optimization strategies

ğŸ”® Future Improvements

Support for real sensor / camera input

Shared memory optimization

Thrust prefix-sum based resampling

Visual particle animation

Multi-GPU scaling

ğŸ¯ Conclusion

This project demonstrates how GPU computing transforms particle filtering from a slow CPU process into a real-time parallel system.
It validates CUDAâ€™s power for scientific computing and robotics applications.

ğŸ‘¤ Author

Utkarsh Mishra
GPU Specialization Capstone Project
